{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLSQRQ9kMCq/Hwq5PB0a5B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# What is BERT?"],"metadata":{"id":"rP5t78W0KbdE"}},{"cell_type":"markdown","source":["- Bidirectional Encoder Representations from Transformers\n","- Bidirection: 양방향\n","- Encoder: 입력값을 숫자의 형태로 바꾸는 모듈을 의미\n","- BERT: 문맥을 양방향으로 이해해서 숫자의 형태로 바꿔주는 딥러닝 모델이다.\n","\n","- 인코더 디코더를 가졌다.\n","- 질적으로 좋은 데이터를 선별하는 능력도 중요하다\n","\n","## Transformer\n","### Encoder\n","1. 데이터 입력\n","2. 포지션 벡터와 결합\n","3. Attention 벡터는 fc로 전송되는데 6번 반복해서 진행된다.\n","4. 최종 출력 값은 트렌스포머 Decoder의 입력값으로 사용된다.\n","\n","**기억해야할 점** 인코더는 모든 토큰을 한 방에 계산한다. 왼쪽에서 오른쪽으로 하나씩 읽어나가는 과정이 없다.\n","\n","### Decoder\n","1. Encoder에서 온 입력값과. 최초 start 스페셜 토큰으로 왼쪽부터 오른쪽으로 출력값을 생성한다.\n","2. Decoder도 Attention 벡터와 fc 결합을 6번 진행한다.\n","3. End 스페셜 토큰이 출력될 때까지 반복한다.\n","\n"],"metadata":{"id":"09Fy2E31Krvg"}},{"cell_type":"code","source":[],"metadata":{"id":"LPM4c1FgOrNE"},"execution_count":null,"outputs":[]}]}