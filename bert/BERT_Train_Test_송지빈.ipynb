{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"11be7dNQPtd6Mxb42JC7t2mDpHO-v7Guk","authorship_tag":"ABX9TyN4z5OBGO6QSJTvz7/DK1cx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"59d548d312cc434a91b4edc804f92818":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8144706f38d543aca4caa246ea2cb3dd","IPY_MODEL_649a5c1b98764380a75af0fd8c465d5f","IPY_MODEL_bfd35328b164419f9a7e106e11ee0923"],"layout":"IPY_MODEL_5be6aa90465240dfb6707e3d28c514db"}},"8144706f38d543aca4caa246ea2cb3dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a61914cc54124ebaa4c532271f421944","placeholder":"​","style":"IPY_MODEL_f7a842211be9411ebfc8ab6681362b1f","value":"  0%"}},"649a5c1b98764380a75af0fd8c465d5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_4eb6ca548bdb4dcb89540c8c1b8342aa","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_983816f06fe7419ca083822805baad69","value":0}},"bfd35328b164419f9a7e106e11ee0923":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b32dbdf975104f7aa60518a286a3dfbc","placeholder":"​","style":"IPY_MODEL_fd3149320c39457ea43c049373b561f0","value":" 0/2 [00:00&lt;?, ?it/s]"}},"5be6aa90465240dfb6707e3d28c514db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a61914cc54124ebaa4c532271f421944":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7a842211be9411ebfc8ab6681362b1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4eb6ca548bdb4dcb89540c8c1b8342aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"983816f06fe7419ca083822805baad69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b32dbdf975104f7aa60518a286a3dfbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd3149320c39457ea43c049373b561f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51e3b294d6434b7d967ffc66fff2c29b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4cc55fc450444e3a2877c0d07ef88e8","IPY_MODEL_a0bb6e5d75a342c3823d887beb66bd5b","IPY_MODEL_866ee54626044317b6388c9ee02c038c"],"layout":"IPY_MODEL_d9fefab1611f4a68b45f86e1faa96978"}},"b4cc55fc450444e3a2877c0d07ef88e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_296b02b32575404eb358fe46169f1398","placeholder":"​","style":"IPY_MODEL_f7738ae504e5435bb9e75e1fb2891acf","value":"Downloading: 100%"}},"a0bb6e5d75a342c3823d887beb66bd5b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_745a90ffdd5a46f4a6e2eada1eb5869d","max":995526,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd773884adcd49b7b71a11be4609143e","value":995526}},"866ee54626044317b6388c9ee02c038c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1a84f5ddea94f20b66514151003c5e4","placeholder":"​","style":"IPY_MODEL_37369c65d4954d2b9b27dd898c72b0df","value":" 996k/996k [00:00&lt;00:00, 1.13MB/s]"}},"d9fefab1611f4a68b45f86e1faa96978":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"296b02b32575404eb358fe46169f1398":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7738ae504e5435bb9e75e1fb2891acf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"745a90ffdd5a46f4a6e2eada1eb5869d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd773884adcd49b7b71a11be4609143e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1a84f5ddea94f20b66514151003c5e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37369c65d4954d2b9b27dd898c72b0df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccd70088d3f54797b602476866bbbf1c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2a262499c2a432f91007815b12d0262","IPY_MODEL_d974062263a94d3f972cb5eb57658946","IPY_MODEL_557db22d65c94c639a7a7bd84618ba6e"],"layout":"IPY_MODEL_5db793a770784c9084aebcf69a97d591"}},"d2a262499c2a432f91007815b12d0262":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_619ec61891d844638e8f2888a9c532ff","placeholder":"​","style":"IPY_MODEL_2858bc72a64246279b1524ac9a666e3f","value":"Downloading: 100%"}},"d974062263a94d3f972cb5eb57658946":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_36ea5d6061d74f1fb5fa93728d91379f","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87de2a6a740c4c278a28546bd6fd6580","value":29}},"557db22d65c94c639a7a7bd84618ba6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12949218339442caa4e6d3fc2e7a64e4","placeholder":"​","style":"IPY_MODEL_5d870f7c220e433881ba95b7f9a6ffc8","value":" 29.0/29.0 [00:00&lt;00:00, 1.05kB/s]"}},"5db793a770784c9084aebcf69a97d591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"619ec61891d844638e8f2888a9c532ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2858bc72a64246279b1524ac9a666e3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36ea5d6061d74f1fb5fa93728d91379f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87de2a6a740c4c278a28546bd6fd6580":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12949218339442caa4e6d3fc2e7a64e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d870f7c220e433881ba95b7f9a6ffc8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"692d55cdeb0945aea3c32e5450b2c571":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf747a613afa4beeb38dca4e9aa451f1","IPY_MODEL_3d2d9b1577444f04ab06bb3185cb40c5","IPY_MODEL_a75fe734223b43dabd21ffcf26c52891"],"layout":"IPY_MODEL_2c460a46061c4c60b32383caea8f0046"}},"bf747a613afa4beeb38dca4e9aa451f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c3451393c724ad892fd18614f441512","placeholder":"​","style":"IPY_MODEL_c69446a58d5d44d283838f6cbeb12f9f","value":"Downloading: 100%"}},"3d2d9b1577444f04ab06bb3185cb40c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcd5b684577b4de4be0a89f2b166d838","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f40c5e0ebb084e3aa5ab0ed886d9fefc","value":625}},"a75fe734223b43dabd21ffcf26c52891":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66a92a1f9c8a45df8757a46d879a50a7","placeholder":"​","style":"IPY_MODEL_7e9fedf58773451fb0f56569602ecc3f","value":" 625/625 [00:00&lt;00:00, 26.9kB/s]"}},"2c460a46061c4c60b32383caea8f0046":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c3451393c724ad892fd18614f441512":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c69446a58d5d44d283838f6cbeb12f9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcd5b684577b4de4be0a89f2b166d838":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f40c5e0ebb084e3aa5ab0ed886d9fefc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66a92a1f9c8a45df8757a46d879a50a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e9fedf58773451fb0f56569602ecc3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Colab 환경 설정"],"metadata":{"id":"OHEPsX_sTz_8"}},{"cell_type":"code","source":["!pip install -q -U \"tensorflow-text==2.8.*\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-GxjsoDTwQc","executionInfo":{"status":"ok","timestamp":1673953967601,"user_tz":-540,"elapsed":50678,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"ab5a004f-c7c9-4d47-e941-833f485bb57a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 KB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -q tf-models-official==2.7.0"],"metadata":{"id":"azBkwT-rbgSa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install pad_sequences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7lDXhVidQ5l","executionInfo":{"status":"ok","timestamp":1674551256130,"user_tz":-540,"elapsed":14906,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"6ca23851-f987-45ce-bb89-78528e5fcb97"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pad_sequences\n","  Downloading pad-sequences-0.6.1.tar.gz (9.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pad_sequences\n","  Building wheel for pad_sequences (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pad_sequences: filename=pad_sequences-0.6.1-py3-none-any.whl size=10216 sha256=59ae51b895da1b1b2f80a01740e058a56c71d5e893776ef266c890135e3dbbe0\n","  Stored in directory: /root/.cache/pip/wheels/51/e4/5c/d3610ed4476515e540ff4096e3e9a8a3e701dedfe1072eb000\n","Successfully built pad_sequences\n","Installing collected packages: pad_sequences\n","Successfully installed pad_sequences-0.6.1\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import torch\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras_preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime"],"metadata":{"id":"5MbYmZYWTsb6","executionInfo":{"status":"ok","timestamp":1674551266525,"user_tz":-540,"elapsed":1126,"user":{"displayName":"송지빈","userId":"11346694006770834807"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["n_devices = torch.cuda.device_count()\n","print(n_devices)\n","\n","for i in range(n_devices):\n","    print(torch.cuda.get_device_name(i))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkLNJ_40dC8b","executionInfo":{"status":"ok","timestamp":1674551270643,"user_tz":-540,"elapsed":806,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"29c05419-8c43-4b0a-90f3-5c66d16cd330"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","Tesla T4\n"]}]},{"cell_type":"markdown","source":["# 데이터셋 불러오기"],"metadata":{"id":"7Sb8WyOjT1mz"}},{"cell_type":"markdown","source":["## 학습 데이터 불러오기"],"metadata":{"id":"evis2aBvd6g9"}},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/동아리_스터디_대외활동/Solux/2학기/Solux_Honey_News/crawling/result_crawling/preprocessed_data.csv')\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":641},"id":"lXF2e8rbT3DJ","executionInfo":{"status":"ok","timestamp":1674551272962,"user_tz":-540,"elapsed":1934,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"66996e4a-b800-4ee5-9ed1-6ab6065e09e2"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                      title  news  \\\n","0         윤 대통령 “북 무인기 한 대 왔으면 우린 2~3대 보내라”  중앙일보   \n","1               노웅래 체포안 부결...與 \"이재명 방탄 예고편\"  중앙일보   \n","2        이재명 때린 17년전 이재명? '지자체 부정부패' 논문 재조명  중앙일보   \n","3    [단독] TBS 직원 10명중 6명 \"김어준 방송, 중립적이지 않다\"  중앙일보   \n","4        미사일 안쏘고 韓 흔든 北…대박난 '회색지대 도발' 더 세진다  중앙일보   \n","..                                      ...   ...   \n","395    43%가 ‘사표’…“소선거구제는 썩은 그릇에 국물 조금 붓는 것”   한겨레   \n","396         윤 대통령 지지율 41.2%…2주 연속 상승 [리얼미터]   한겨레   \n","397   민주, ‘이재명 수사’ 검사 16명 공개…국힘 “헌법 질서에 도전”   한겨레   \n","398         노란봉투법·양곡관리법·차별금지법…여야 이견 속 해 넘길판   한겨레   \n","399       ‘국민통합’ 꺼내든 정치인 사면…이번에도 국민 공감대는 없다   한겨레   \n","\n","                                           new_article  \n","0    윤석열 대통령 28일 북한 도발 에도 확실하게 응징 보복 라 그것 가장 강력한 수단...  \n","1    저번 주셨는데 뭘 주냐 저번 그거 잘 쓰고 있는데 라고 말 하는 노웅래 민주당 의원...  \n","2    이재명 민주당 대표 성남 FC 후 원금 의혹 관련 해 검찰 수사 받는 가운데 17년...  \n","3    정치 적 편향 논란 끊이지 않던 TBS 교통 방송 라디오 프로그램 김어준 뉴스 공장...  \n","4    북한 대남 도발 날로 교묘해지고 핵 ㆍ 미사일 같은 고 강도 도발 에다 지난 26일...  \n","..                                                 ...  \n","395  2020년 치러진 21 대 총선 참여 한 유권자 2874만 1408 표 가운데 10...  \n","396  윤석열 대통령 국정 지지율 2 주 연속 상승 해 41 대를 기록 했다 리얼미터 미디...  \n","397  민주당 이재명 대표 관련 한 수사 진행 중인 검사 사진 이름 담긴 자료 만들어 전국...  \n","398  여야 지난 24일 638조 7천억원 규모 내년 도 예산안 부수 법안 지각 처리 했지...  \n","399  윤석열 대통령 연말 특별사면 권 행사 앞두고 헌법 보장 된 대통령 사면권 에는 국민...  \n","\n","[400 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-02bcd616-0e6d-4f11-98b6-22cab3f77f30\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>news</th>\n","      <th>new_article</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>윤 대통령 “북 무인기 한 대 왔으면 우린 2~3대 보내라”</td>\n","      <td>중앙일보</td>\n","      <td>윤석열 대통령 28일 북한 도발 에도 확실하게 응징 보복 라 그것 가장 강력한 수단...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>노웅래 체포안 부결...與 \"이재명 방탄 예고편\"</td>\n","      <td>중앙일보</td>\n","      <td>저번 주셨는데 뭘 주냐 저번 그거 잘 쓰고 있는데 라고 말 하는 노웅래 민주당 의원...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>이재명 때린 17년전 이재명? '지자체 부정부패' 논문 재조명</td>\n","      <td>중앙일보</td>\n","      <td>이재명 민주당 대표 성남 FC 후 원금 의혹 관련 해 검찰 수사 받는 가운데 17년...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[단독] TBS 직원 10명중 6명 \"김어준 방송, 중립적이지 않다\"</td>\n","      <td>중앙일보</td>\n","      <td>정치 적 편향 논란 끊이지 않던 TBS 교통 방송 라디오 프로그램 김어준 뉴스 공장...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>미사일 안쏘고 韓 흔든 北…대박난 '회색지대 도발' 더 세진다</td>\n","      <td>중앙일보</td>\n","      <td>북한 대남 도발 날로 교묘해지고 핵 ㆍ 미사일 같은 고 강도 도발 에다 지난 26일...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>395</th>\n","      <td>43%가 ‘사표’…“소선거구제는 썩은 그릇에 국물 조금 붓는 것”</td>\n","      <td>한겨레</td>\n","      <td>2020년 치러진 21 대 총선 참여 한 유권자 2874만 1408 표 가운데 10...</td>\n","    </tr>\n","    <tr>\n","      <th>396</th>\n","      <td>윤 대통령 지지율 41.2%…2주 연속 상승 [리얼미터]</td>\n","      <td>한겨레</td>\n","      <td>윤석열 대통령 국정 지지율 2 주 연속 상승 해 41 대를 기록 했다 리얼미터 미디...</td>\n","    </tr>\n","    <tr>\n","      <th>397</th>\n","      <td>민주, ‘이재명 수사’ 검사 16명 공개…국힘 “헌법 질서에 도전”</td>\n","      <td>한겨레</td>\n","      <td>민주당 이재명 대표 관련 한 수사 진행 중인 검사 사진 이름 담긴 자료 만들어 전국...</td>\n","    </tr>\n","    <tr>\n","      <th>398</th>\n","      <td>노란봉투법·양곡관리법·차별금지법…여야 이견 속 해 넘길판</td>\n","      <td>한겨레</td>\n","      <td>여야 지난 24일 638조 7천억원 규모 내년 도 예산안 부수 법안 지각 처리 했지...</td>\n","    </tr>\n","    <tr>\n","      <th>399</th>\n","      <td>‘국민통합’ 꺼내든 정치인 사면…이번에도 국민 공감대는 없다</td>\n","      <td>한겨레</td>\n","      <td>윤석열 대통령 연말 특별사면 권 행사 앞두고 헌법 보장 된 대통령 사면권 에는 국민...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>400 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02bcd616-0e6d-4f11-98b6-22cab3f77f30')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-02bcd616-0e6d-4f11-98b6-22cab3f77f30 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-02bcd616-0e6d-4f11-98b6-22cab3f77f30');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["보수 정당: 1 조선일보, 중앙일보\n","\n","진보 정당: 0 한겨례, 경향"],"metadata":{"id":"8Z3KMW6Nt69w"}},{"cell_type":"code","source":["df.loc[(df['news'] == '조선일보') | (df['news'] == '중앙일보'), 'news'] = 1"],"metadata":{"id":"yXKxZKgiuGY3","executionInfo":{"status":"ok","timestamp":1674551272968,"user_tz":-540,"elapsed":45,"user":{"displayName":"송지빈","userId":"11346694006770834807"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["df.loc[(df['news'] == '한겨레') | (df['news'] == '경향'), 'news'] = 0\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":641},"id":"47lXXwWuxxyQ","executionInfo":{"status":"ok","timestamp":1674551272973,"user_tz":-540,"elapsed":49,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"2cdc40ca-7198-493e-f69a-c00320088cc7"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                      title news  \\\n","0         윤 대통령 “북 무인기 한 대 왔으면 우린 2~3대 보내라”    1   \n","1               노웅래 체포안 부결...與 \"이재명 방탄 예고편\"    1   \n","2        이재명 때린 17년전 이재명? '지자체 부정부패' 논문 재조명    1   \n","3    [단독] TBS 직원 10명중 6명 \"김어준 방송, 중립적이지 않다\"    1   \n","4        미사일 안쏘고 韓 흔든 北…대박난 '회색지대 도발' 더 세진다    1   \n","..                                      ...  ...   \n","395    43%가 ‘사표’…“소선거구제는 썩은 그릇에 국물 조금 붓는 것”    0   \n","396         윤 대통령 지지율 41.2%…2주 연속 상승 [리얼미터]    0   \n","397   민주, ‘이재명 수사’ 검사 16명 공개…국힘 “헌법 질서에 도전”    0   \n","398         노란봉투법·양곡관리법·차별금지법…여야 이견 속 해 넘길판    0   \n","399       ‘국민통합’ 꺼내든 정치인 사면…이번에도 국민 공감대는 없다    0   \n","\n","                                           new_article  \n","0    윤석열 대통령 28일 북한 도발 에도 확실하게 응징 보복 라 그것 가장 강력한 수단...  \n","1    저번 주셨는데 뭘 주냐 저번 그거 잘 쓰고 있는데 라고 말 하는 노웅래 민주당 의원...  \n","2    이재명 민주당 대표 성남 FC 후 원금 의혹 관련 해 검찰 수사 받는 가운데 17년...  \n","3    정치 적 편향 논란 끊이지 않던 TBS 교통 방송 라디오 프로그램 김어준 뉴스 공장...  \n","4    북한 대남 도발 날로 교묘해지고 핵 ㆍ 미사일 같은 고 강도 도발 에다 지난 26일...  \n","..                                                 ...  \n","395  2020년 치러진 21 대 총선 참여 한 유권자 2874만 1408 표 가운데 10...  \n","396  윤석열 대통령 국정 지지율 2 주 연속 상승 해 41 대를 기록 했다 리얼미터 미디...  \n","397  민주당 이재명 대표 관련 한 수사 진행 중인 검사 사진 이름 담긴 자료 만들어 전국...  \n","398  여야 지난 24일 638조 7천억원 규모 내년 도 예산안 부수 법안 지각 처리 했지...  \n","399  윤석열 대통령 연말 특별사면 권 행사 앞두고 헌법 보장 된 대통령 사면권 에는 국민...  \n","\n","[400 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-f8849f91-6298-4a0a-8020-8d084ecde5a8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>news</th>\n","      <th>new_article</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>윤 대통령 “북 무인기 한 대 왔으면 우린 2~3대 보내라”</td>\n","      <td>1</td>\n","      <td>윤석열 대통령 28일 북한 도발 에도 확실하게 응징 보복 라 그것 가장 강력한 수단...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>노웅래 체포안 부결...與 \"이재명 방탄 예고편\"</td>\n","      <td>1</td>\n","      <td>저번 주셨는데 뭘 주냐 저번 그거 잘 쓰고 있는데 라고 말 하는 노웅래 민주당 의원...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>이재명 때린 17년전 이재명? '지자체 부정부패' 논문 재조명</td>\n","      <td>1</td>\n","      <td>이재명 민주당 대표 성남 FC 후 원금 의혹 관련 해 검찰 수사 받는 가운데 17년...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[단독] TBS 직원 10명중 6명 \"김어준 방송, 중립적이지 않다\"</td>\n","      <td>1</td>\n","      <td>정치 적 편향 논란 끊이지 않던 TBS 교통 방송 라디오 프로그램 김어준 뉴스 공장...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>미사일 안쏘고 韓 흔든 北…대박난 '회색지대 도발' 더 세진다</td>\n","      <td>1</td>\n","      <td>북한 대남 도발 날로 교묘해지고 핵 ㆍ 미사일 같은 고 강도 도발 에다 지난 26일...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>395</th>\n","      <td>43%가 ‘사표’…“소선거구제는 썩은 그릇에 국물 조금 붓는 것”</td>\n","      <td>0</td>\n","      <td>2020년 치러진 21 대 총선 참여 한 유권자 2874만 1408 표 가운데 10...</td>\n","    </tr>\n","    <tr>\n","      <th>396</th>\n","      <td>윤 대통령 지지율 41.2%…2주 연속 상승 [리얼미터]</td>\n","      <td>0</td>\n","      <td>윤석열 대통령 국정 지지율 2 주 연속 상승 해 41 대를 기록 했다 리얼미터 미디...</td>\n","    </tr>\n","    <tr>\n","      <th>397</th>\n","      <td>민주, ‘이재명 수사’ 검사 16명 공개…국힘 “헌법 질서에 도전”</td>\n","      <td>0</td>\n","      <td>민주당 이재명 대표 관련 한 수사 진행 중인 검사 사진 이름 담긴 자료 만들어 전국...</td>\n","    </tr>\n","    <tr>\n","      <th>398</th>\n","      <td>노란봉투법·양곡관리법·차별금지법…여야 이견 속 해 넘길판</td>\n","      <td>0</td>\n","      <td>여야 지난 24일 638조 7천억원 규모 내년 도 예산안 부수 법안 지각 처리 했지...</td>\n","    </tr>\n","    <tr>\n","      <th>399</th>\n","      <td>‘국민통합’ 꺼내든 정치인 사면…이번에도 국민 공감대는 없다</td>\n","      <td>0</td>\n","      <td>윤석열 대통령 연말 특별사면 권 행사 앞두고 헌법 보장 된 대통령 사면권 에는 국민...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>400 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8849f91-6298-4a0a-8020-8d084ecde5a8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f8849f91-6298-4a0a-8020-8d084ecde5a8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f8849f91-6298-4a0a-8020-8d084ecde5a8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["df.news.unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0kGL5ZIw0pn","executionInfo":{"status":"ok","timestamp":1674551272977,"user_tz":-540,"elapsed":51,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"ad8dd586-4da5-44cf-b047-7d47006e2de2"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0], dtype=object)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["df['news'] = df['news'].astype(int)\n","df['news']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NXcStwrb3qj","executionInfo":{"status":"ok","timestamp":1674551272979,"user_tz":-540,"elapsed":45,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"964b9245-e81c-4030-c9b6-670986e3e45d"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0      1\n","1      1\n","2      1\n","3      1\n","4      1\n","      ..\n","395    0\n","396    0\n","397    0\n","398    0\n","399    0\n","Name: news, Length: 400, dtype: int64"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## Train set / Test set으로 나누기"],"metadata":{"id":"WleGsMVNd9Nm"}},{"cell_type":"code","source":["# train dataset\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(df[['title', 'new_article']], df['news'])\n","# X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n","\n","train = pd.concat([X_train,y_train],1)\n","test = pd.concat([X_test,y_test],1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zqx-Q5zRUHqM","executionInfo":{"status":"ok","timestamp":1674551274399,"user_tz":-540,"elapsed":8,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"0349b168-dd3e-4f94-b090-c01c96f8a97c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-505e853eba5d>:7: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n","  train = pd.concat([X_train,y_train],1)\n","<ipython-input-12-505e853eba5d>:8: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n","  test = pd.concat([X_test,y_test],1)\n"]}]},{"cell_type":"markdown","source":["# Train set 전처리"],"metadata":{"id":"iWMaBMgrewnJ"}},{"cell_type":"markdown","source":["## 각 문장마다 [CLS]와 [SEP]를 붙여주기"],"metadata":{"id":"RVJ-bmsieyo-"}},{"cell_type":"code","source":["sentences = [\"[CLS] \" + str(s) + \" [SEP]\" for s in train.new_article]\n","sentences[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9py4IU-hfDKj","executionInfo":{"status":"ok","timestamp":1674551276581,"user_tz":-540,"elapsed":9,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"8855ce61-59b6-44e0-8759-0bcb7fe22636"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 여야 28일 국회 국방 위원회 전체 회의 열고 국방부 지난 26일 북한 무인기 도발 대한 긴급 현안 보고 받았다 국민 힘 은 문재인 정부 인 2018년 체결 된 9 19 남북 군 합의 군 대응 역량 약화 시켰다고 주장 했다 민주당 은 윤석열 대통령 책임 방기 한 채 문재인 정부 탓 만 하고 있다고 질타 했다 윤 대통령 확전 발언 적절 성 대한 공 방도 이어졌다 날 국방 위 회의 에는 이종섭 국방부 장관 김승겸 합동 참모 본부 의장 출석 했다 김영배 민주당 의원 은 도둑 들었는데 경찰 엉뚱한 짓 거나 범인 놓친 이유 엉뚱하게 대니까 문제 라며 대나 되는 무인기 대한민국 영토 6시간 돌아다녔는데도 제대로 사과 거나 사퇴 거나 책임지겠다는 사람 없다 고 윤 대통령 전 정부 책임 론 비판 했다 같은 당 설훈 의원 도 윤 대통령 사과 요구 했다 민주당 의원 은 윤 대통령 문재인 정부 북한 무인기 대응 훈련 대비 없었다고 한 데 대해 사실 아니라고 강조 했다 국방 위 야당 간사 인 김병주 의원 은 문재인 정부 5년 엄청나게 대 무인기 체계 발전 시켰다 5년 전 에는 장비 없어서 탐지 못 했던 걸 탐지 한 라며 국군 통 수권 훈련 전혀 안 했다라는 소리 하는 너무 놀랐다 고 말 했다 장관 은 적 상황 상정 한 실전 적 인 훈련 대해 서는 취약 했다 며 특히 합참 주도 모든 자산 통합 해서 운용 하는 차원 훈련 은 없었다는 점 전무하다 는 표현 사용 된 이해해 달라 고 윤 대통령 옹호 했다 윤 대통령 확전 각오 발언 국가 안전보장 회의 NSC 개최 하지 않은 도 비판 올랐다 설훈 의원 은 확전 각오 하고 무인기 올려 보내라고 대통령 지시 했다는데 저녁 송년회 했다 게 앞뒤 맞느냐 며 대통령 전투복 입고 벙커 상황 지켜보고 있어야 상식 맞는 얘기 라고 말 했다 송옥 주 민주당 의원 은 대통령 국민 불안 증폭 시킨다고 하면 정말 무책임한 이라고 밝혔다 장관 은 에도 전쟁 일어나지 않도록 하고 도발 하지 못 하도록 억제 하기 위 한 각오 대비 해야 한다는 라고 말 했다 민주당 은 북한 무인기 용산 대통령실 촬영 했을 가능성 제기 했다 김영배 의원 은 국방부 제공 한 북한 무인기 항로 근거 그림 만 보면 용산 지나간 고 말 했다 설훈 의원 은 북한 용산 찍은 용산 대통령실 사진 내놓으면 할 이냐 고 추궁 했다 장관 은 용산 까지는 오지 않았던 은 확신 한다 단계 별로 감시자 산들 다 확인 된다 고 답 했다 국민 힘 은 북한 무인기 격추 실패 문재인 정부 책임 있다며 내 로남불 이라고 주장 했다 국방 위 여당 간사 인 신원식 의원 은 드디어 문재인 정권 잘 못 된 안보 정책 참담한 성적표 배달 됐다 고 말 했다 신 의원 은 더 심각한 게 9 19 합의 라며 북한 은 대남 도발 역량 자유롭게 전방 위로 강화했는데 는 손발 꽁꽁 묶었다 고 말 했다 신 의원 은 9 19 합의 설정 된 비행 금지 구역 남 측 무인기 운용 극도 제한 하고 있다며 무인기 배치 들어간 예산 4100억원 그냥 사장 시킨 이다 김정은 한테 잘 보이기 위해 서 라고 말 했다 같은 당 성 일종 의원 은 9 19 합의 문재인 정부 GP 일반전초 다 헐어 냈다 감시 자산 설치 할 수가 없는 라며 군 해체 수준 가게 된 은 문재인 정부 오는 거 축적 된 이지 출범 한 지 6 개월 밖에 안 된 윤석열 정부 책임 돌린다는 부끄러운 줄 알라 고 말 했다 민주당 의원 반발 양 당 고성 오갔다 국민 힘 은 윤 대통령 NSC 개최 하지 않은 적절한 판단 이었다는 입장 도 밝혔다 한기호 의원 은 작전 진행 하고 있을 는 지휘 관 모든 위임 하는 라며 NSC 하면 작전 지휘 대통령 하는 결과 된다 호미로 막 걸 가래 막는 이라고 말 했다 신 의원 은 군 대응 조치 북한 상공 무인기 침투 시킨 데 대해 기념비 적 인 사건 윤석열 정부 휴전 이후 북한 도발 역사 종지부 찍 수 있는 제대로 된 창 썼다 고 극찬 했다 민주당 에서는 대통령실 용산 국방부 청사 이전 합참 흩어져 전해 국 지 전 대응 우려 커졌다는 주장 도 나왔다 국민 힘 은 민주당 이재명 대표 방탄 위해 사건 정치 적 이용 한 다 고 맞섰다 장관 은 북한 무인기 영공 침범 보고 받은 언 인지 묻는 민주당 의원 처음 12시 10분 이라고 답 했다가 이후 11시 50분 이라고 말 했다 윤 대통령 에게는 12시 12분 전화 보고 했다고 장관 은 밝혔다 윤 대통령 보고 시점 은 군 무인기 처음 탐지 한 오전 10시 25분 으로부터 1시간 47분 지난 다 장관 은 초기 TOD 열 상감 시 장비 레이더 잡히는 인지 분석 하는 데 필요하지 않겠느냐 고 말 했다 합참 은 국방 위 보고 적 소형 무인기 도발 양상 고려 해 합참 차원 통합 된 실전 적 교육 훈련 강화할 라며 29일 합동 방공 훈련 실시 할 계획 이라고 밝혔다 훈련 에서는 북한 소형 무인기 도발 상황 대비 한 급 부대 별 탐지 타격 자산 운용 능력 중점 점검 할 보인다 [SEP]',\n"," '[CLS] 김지호 기자 경기도 파주시 임진각 있는 한반도 생태 평화 종합 관광 센터 27일 DMZ 평화 관광 잠정 통제 알리는 안 내 문이 붙어 북한 군용 무인기 지난 26일 서울 북부 경기도 김포 파주 인천 강화도 일대 영공 침범 한 데 따른 조치 다 [SEP]',\n"," '[CLS] 유상범 국민 힘 의원 은 27일 극우 유튜버들 잇따른 당 최고 위원 출마 선언 관련 해 경선 흥행 정도 는 도움 될 이라고 말 했다 유 의원 은 날 문화 방송 MBC 라디오 김종배 시선집중 인터뷰 내년 3월 8일 전당대회 앞두고 강신 업 김세의 신혜식 극우 성향 유튜버들 최고 위원 도전 의사 밝힌 두고 국민 적 관심 도 받을 며 이렇게 말 했다 유 의원 은 가세 연 가로세로 연구소 김세의 대표 같은 경우 는 정도 이제 셀럽 셀러브리티 화 돼 있는 분 아니겠냐 며 당원 보수 지지 했던 사람 입장 보면 분들 사이다 발언 통해 서 답답한 심정 대변 해 선호 했다 고 말 했다 유 의원 은 지난 6월 지방선거 경 기 지사 출마 했다가 낙선 한 강용석 변호사 사례 언급 하며 실제 당선 가능성 은 낮다고 봤다 유 의원 은 당 심 판단 할 까는 다르게 본다 며 이분 실제 당 들어와서 당 이끌어 갈 당원 선호 하는 투표 연결 될까 그건 인기 표심 가성 면 에서는 차이 있다고 본다 고 했다 앞서 국민 힘 전당대회 룰 당원 투표 100 확정 한 가운데 극우 성향 유튜브 가로세로 연구소 김세의 대표 최고 위원 김건희 여사 팬클럽 건희 사랑 전 회장 이자 유튜브 강신 업 티브이 TV 운영 하는 강신 업 변호사 는 당 대표 출마 의사 밝힌 바 극우 성향 전광훈 사랑 교회 담임 목사 지난 달 간 유튜브 채널 중심 국민 힘 점령 운동 벌이 며 입당 수 늘리 기도 했다 [SEP]',\n"," '[CLS] 이재명 민주당 대표 취약 지역 시 도당 대한 지원 늘렸다 민주당 전국 정당화 위해 장기 적 이고 체계 적 인 지 원 체계 만들겠다는 이다 대표 는 30일 페이스북 올린 글 권리 당원 납부 하는 반 당비 배분 율 중앙 당 취약 지역 배분 기존 20 30 확대 했다 고 밝혔다 는 이어 나아가 취약 지역 배분 된 당비 사용 처 확대 해 원외 지역 위원회 예산 정책 기능 강화 되도록 했다 고 적었다 그러면서 재정 넉 넉 지 않은 시 도당 포함 해 험지 고생 하시는 원외 위원 장님 께 이나마 도움 되길 바란다 고 덧붙였다 대표 는 원외 위원장 대한 지원 확대 는 여러 최고 위원 공통 공약 이기도 하다 며 당비 배분 조치 넘어 취약 지역 대한 더 적극 적 인 지원 방안 모색 해 나가겠다 고 했다 는 지역 정치 시작 했기에 잘 알 고 며 지역 시 도당 이라는 뿌리 튼튼한 정당 이어야 집권 이라는 열매 맺을 수 고 썼다 [SEP]',\n"," '[CLS] 박지현 전 민주당 비상 대책 위원 장이 이재명 대표 대한 검찰 수사 관련 해 당 은 민생 일치단결 하고 이재명 대표 는 개인 대응 해야 한다 고 말 했다 민주당 대표 경선 출마 불발 된 뒤 정치 활동 잠정 중단 한 박 전 비대 위원장 은 최근 저서 이상한 나라 박지현 펴내며 활동 재개 했다 박 전 비대 위원장 은 26일 시비 에스 CBS 라디오 인터뷰 지금 처럼 대표 사법 리스크 터질 걸 예상 못 한 사람 은 없다 이럴 걸 알았기 때문 대표 당 대표 출마 하면 안 된다고 계속 얘기 했던 라며 이같이 밝혔다 는 대표 검찰 소환조사 대해 선 안 나갈 수 없는 문제 다 본인 당당하면 당당하게 검찰 조사 하고 이야기 하고 나오면 되는 거 다 라고 말 했다 는 대표 수사 대한 당 차원 대응 에도 비판 적 인 입장 내놨다 박 전 비대 위원장 은 이재명 대표 는 개인 사법 리스크 대응 하고 당 은 민생 일치단결 해서 민생 이야기 만 나가야 한다 이재명 대표 서 대응 해야 한다 고 밝혔다 대표 대표 직 사퇴 필요성 대해 선 국민 당원 뽑은 선출 된 당 대표 다 그렇기 때문 대표 직 내려놓는 은 지금 역할 은 아니라고 생각 한다 고 선 그었다 박 전 비대 위원장 은 앞 으로의 계획 대해 책 매개 전국 다니면서 국민 분들 당원 분들 만나 여러 이야기 들어 보려고 한다 며 2024년 총선 생각 한 다기 보다는 장기 적 정치 하려는 마음 가지 고 고 말 했다 [SEP]']"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## 서브워드 토크나이저: WordPiece"],"metadata":{"id":"4jo99b2dkspr"}},{"cell_type":"code","source":["# 샘플 토크나이징\n","import pandas as pd\n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", do_lower_case=False)\n","result = tokenizer.tokenize('안녕하세요')\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["51e3b294d6434b7d967ffc66fff2c29b","b4cc55fc450444e3a2877c0d07ef88e8","a0bb6e5d75a342c3823d887beb66bd5b","866ee54626044317b6388c9ee02c038c","d9fefab1611f4a68b45f86e1faa96978","296b02b32575404eb358fe46169f1398","f7738ae504e5435bb9e75e1fb2891acf","745a90ffdd5a46f4a6e2eada1eb5869d","cd773884adcd49b7b71a11be4609143e","c1a84f5ddea94f20b66514151003c5e4","37369c65d4954d2b9b27dd898c72b0df","ccd70088d3f54797b602476866bbbf1c","d2a262499c2a432f91007815b12d0262","d974062263a94d3f972cb5eb57658946","557db22d65c94c639a7a7bd84618ba6e","5db793a770784c9084aebcf69a97d591","619ec61891d844638e8f2888a9c532ff","2858bc72a64246279b1524ac9a666e3f","36ea5d6061d74f1fb5fa93728d91379f","87de2a6a740c4c278a28546bd6fd6580","12949218339442caa4e6d3fc2e7a64e4","5d870f7c220e433881ba95b7f9a6ffc8","692d55cdeb0945aea3c32e5450b2c571","bf747a613afa4beeb38dca4e9aa451f1","3d2d9b1577444f04ab06bb3185cb40c5","a75fe734223b43dabd21ffcf26c52891","2c460a46061c4c60b32383caea8f0046","8c3451393c724ad892fd18614f441512","c69446a58d5d44d283838f6cbeb12f9f","fcd5b684577b4de4be0a89f2b166d838","f40c5e0ebb084e3aa5ab0ed886d9fefc","66a92a1f9c8a45df8757a46d879a50a7","7e9fedf58773451fb0f56569602ecc3f"]},"id":"tr8gMKuClGLZ","executionInfo":{"status":"ok","timestamp":1674551278676,"user_tz":-540,"elapsed":1809,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"90351f4f-1f52-46e4-b0cd-ad21859247b2"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e3b294d6434b7d967ffc66fff2c29b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccd70088d3f54797b602476866bbbf1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"692d55cdeb0945aea3c32e5450b2c571"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["['안', '##녕', '##하', '##세', '##요']\n"]}]},{"cell_type":"code","source":["# 전체 데이터에 토크나이징 수행\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(s) for s in sentences]"],"metadata":{"id":"X-kE8PwurB0P","executionInfo":{"status":"ok","timestamp":1674551281422,"user_tz":-540,"elapsed":2755,"user":{"displayName":"송지빈","userId":"11346694006770834807"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# 토크나이징 잘 되었는지 확인하기\n","print(sentences[0])  #토크나이징 전\n","print(tokenized_texts[0]) #토크나이징 후"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgDk8dr6rPEM","executionInfo":{"status":"ok","timestamp":1674551281982,"user_tz":-540,"elapsed":11,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"2ba3980b-fe25-4c18-9c6d-55b787be9279"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] 여야 28일 국회 국방 위원회 전체 회의 열고 국방부 지난 26일 북한 무인기 도발 대한 긴급 현안 보고 받았다 국민 힘 은 문재인 정부 인 2018년 체결 된 9 19 남북 군 합의 군 대응 역량 약화 시켰다고 주장 했다 민주당 은 윤석열 대통령 책임 방기 한 채 문재인 정부 탓 만 하고 있다고 질타 했다 윤 대통령 확전 발언 적절 성 대한 공 방도 이어졌다 날 국방 위 회의 에는 이종섭 국방부 장관 김승겸 합동 참모 본부 의장 출석 했다 김영배 민주당 의원 은 도둑 들었는데 경찰 엉뚱한 짓 거나 범인 놓친 이유 엉뚱하게 대니까 문제 라며 대나 되는 무인기 대한민국 영토 6시간 돌아다녔는데도 제대로 사과 거나 사퇴 거나 책임지겠다는 사람 없다 고 윤 대통령 전 정부 책임 론 비판 했다 같은 당 설훈 의원 도 윤 대통령 사과 요구 했다 민주당 의원 은 윤 대통령 문재인 정부 북한 무인기 대응 훈련 대비 없었다고 한 데 대해 사실 아니라고 강조 했다 국방 위 야당 간사 인 김병주 의원 은 문재인 정부 5년 엄청나게 대 무인기 체계 발전 시켰다 5년 전 에는 장비 없어서 탐지 못 했던 걸 탐지 한 라며 국군 통 수권 훈련 전혀 안 했다라는 소리 하는 너무 놀랐다 고 말 했다 장관 은 적 상황 상정 한 실전 적 인 훈련 대해 서는 취약 했다 며 특히 합참 주도 모든 자산 통합 해서 운용 하는 차원 훈련 은 없었다는 점 전무하다 는 표현 사용 된 이해해 달라 고 윤 대통령 옹호 했다 윤 대통령 확전 각오 발언 국가 안전보장 회의 NSC 개최 하지 않은 도 비판 올랐다 설훈 의원 은 확전 각오 하고 무인기 올려 보내라고 대통령 지시 했다는데 저녁 송년회 했다 게 앞뒤 맞느냐 며 대통령 전투복 입고 벙커 상황 지켜보고 있어야 상식 맞는 얘기 라고 말 했다 송옥 주 민주당 의원 은 대통령 국민 불안 증폭 시킨다고 하면 정말 무책임한 이라고 밝혔다 장관 은 에도 전쟁 일어나지 않도록 하고 도발 하지 못 하도록 억제 하기 위 한 각오 대비 해야 한다는 라고 말 했다 민주당 은 북한 무인기 용산 대통령실 촬영 했을 가능성 제기 했다 김영배 의원 은 국방부 제공 한 북한 무인기 항로 근거 그림 만 보면 용산 지나간 고 말 했다 설훈 의원 은 북한 용산 찍은 용산 대통령실 사진 내놓으면 할 이냐 고 추궁 했다 장관 은 용산 까지는 오지 않았던 은 확신 한다 단계 별로 감시자 산들 다 확인 된다 고 답 했다 국민 힘 은 북한 무인기 격추 실패 문재인 정부 책임 있다며 내 로남불 이라고 주장 했다 국방 위 여당 간사 인 신원식 의원 은 드디어 문재인 정권 잘 못 된 안보 정책 참담한 성적표 배달 됐다 고 말 했다 신 의원 은 더 심각한 게 9 19 합의 라며 북한 은 대남 도발 역량 자유롭게 전방 위로 강화했는데 는 손발 꽁꽁 묶었다 고 말 했다 신 의원 은 9 19 합의 설정 된 비행 금지 구역 남 측 무인기 운용 극도 제한 하고 있다며 무인기 배치 들어간 예산 4100억원 그냥 사장 시킨 이다 김정은 한테 잘 보이기 위해 서 라고 말 했다 같은 당 성 일종 의원 은 9 19 합의 문재인 정부 GP 일반전초 다 헐어 냈다 감시 자산 설치 할 수가 없는 라며 군 해체 수준 가게 된 은 문재인 정부 오는 거 축적 된 이지 출범 한 지 6 개월 밖에 안 된 윤석열 정부 책임 돌린다는 부끄러운 줄 알라 고 말 했다 민주당 의원 반발 양 당 고성 오갔다 국민 힘 은 윤 대통령 NSC 개최 하지 않은 적절한 판단 이었다는 입장 도 밝혔다 한기호 의원 은 작전 진행 하고 있을 는 지휘 관 모든 위임 하는 라며 NSC 하면 작전 지휘 대통령 하는 결과 된다 호미로 막 걸 가래 막는 이라고 말 했다 신 의원 은 군 대응 조치 북한 상공 무인기 침투 시킨 데 대해 기념비 적 인 사건 윤석열 정부 휴전 이후 북한 도발 역사 종지부 찍 수 있는 제대로 된 창 썼다 고 극찬 했다 민주당 에서는 대통령실 용산 국방부 청사 이전 합참 흩어져 전해 국 지 전 대응 우려 커졌다는 주장 도 나왔다 국민 힘 은 민주당 이재명 대표 방탄 위해 사건 정치 적 이용 한 다 고 맞섰다 장관 은 북한 무인기 영공 침범 보고 받은 언 인지 묻는 민주당 의원 처음 12시 10분 이라고 답 했다가 이후 11시 50분 이라고 말 했다 윤 대통령 에게는 12시 12분 전화 보고 했다고 장관 은 밝혔다 윤 대통령 보고 시점 은 군 무인기 처음 탐지 한 오전 10시 25분 으로부터 1시간 47분 지난 다 장관 은 초기 TOD 열 상감 시 장비 레이더 잡히는 인지 분석 하는 데 필요하지 않겠느냐 고 말 했다 합참 은 국방 위 보고 적 소형 무인기 도발 양상 고려 해 합참 차원 통합 된 실전 적 교육 훈련 강화할 라며 29일 합동 방공 훈련 실시 할 계획 이라고 밝혔다 훈련 에서는 북한 소형 무인기 도발 상황 대비 한 급 부대 별 탐지 타격 자산 운용 능력 중점 점검 할 보인다 [SEP]\n","['[CLS]', '여', '##야', '28일', '국', '##회', '국', '##방', '위', '##원', '##회', '전체', '회', '##의', '열', '##고', '국', '##방', '##부', '지', '##난', '26일', '북', '##한', '무', '##인', '##기', '도', '##발', '대한', '긴', '##급', '현', '##안', '보고', '받았다', '국', '##민', '힘', '은', '문', '##재', '##인', '정', '##부', '인', '2018년', '체', '##결', '된', '9', '19', '남', '##북', '군', '합', '##의', '군', '대', '##응', '역', '##량', '약', '##화', '시', '##켰다', '##고', '주', '##장', '했다', '민', '##주', '##당', '은', '윤', '##석', '##열', '대통령', '책', '##임', '방', '##기', '한', '채', '문', '##재', '##인', '정', '##부', '탓', '만', '하고', '있다고', '질', '##타', '했다', '윤', '대통령', '확', '##전', '발', '##언', '적', '##절', '성', '대한', '공', '방', '##도', '이어', '##졌다', '날', '국', '##방', '위', '회', '##의', '에', '##는', '이', '##종', '##섭', '국', '##방', '##부', '장', '##관', '김', '##승', '##겸', '합', '##동', '참', '##모', '본', '##부', '의', '##장', '출', '##석', '했다', '김', '##영', '##배', '민', '##주', '##당', '의', '##원', '은', '도', '##둑', '들', '##었는데', '경', '##찰', '엉', '##뚱', '##한', '짓', '거', '##나', '범', '##인', '놓', '##친', '이', '##유', '엉', '##뚱', '##하게', '대', '##니', '##까', '문', '##제', '라', '##며', '대', '##나', '되는', '무', '##인', '##기', '대한민국', '영', '##토', '6', '##시간', '돌', '##아', '##다', '##녔', '##는데', '##도', '제', '##대로', '사', '##과', '거', '##나', '사', '##퇴', '거', '##나', '책', '##임', '##지', '##겠', '##다는', '사', '##람', '없다', '고', '윤', '대통령', '전', '정', '##부', '책', '##임', '론', '비', '##판', '했다', '같은', '당', '설', '##훈', '의', '##원', '도', '윤', '대통령', '사', '##과', '요', '##구', '했다', '민', '##주', '##당', '의', '##원', '은', '윤', '대통령', '문', '##재', '##인', '정', '##부', '북', '##한', '무', '##인', '##기', '대', '##응', '훈', '##련', '대', '##비', '없었다', '##고', '한', '데', '대해', '사', '##실', '아니라', '##고', '강', '##조', '했다', '국', '##방', '위', '야', '##당', '간', '##사', '인', '김', '##병', '##주', '의', '##원', '은', '문', '##재', '##인', '정', '##부', '5', '##년', '엄', '##청', '##나', '##게', '대', '무', '##인', '##기', '체', '##계', '발', '##전', '시', '##켰다', '5', '##년', '전', '에', '##는', '장', '##비', '없', '##어', '##서', '탐', '##지', '못', '했', '##던', '걸', '탐', '##지', '한', '라', '##며', '국', '##군', '통', '수', '##권', '훈', '##련', '전', '##혀', '안', '했다', '##라는', '소', '##리', '하는', '너', '##무', '놀', '##랐다', '고', '말', '했다', '장', '##관', '은', '적', '상', '##황', '상', '##정', '한', '실', '##전', '적', '인', '훈', '##련', '대해', '서', '##는', '취', '##약', '했다', '며', '특히', '합', '##참', '주', '##도', '모든', '자', '##산', '통', '##합', '해', '##서', '운', '##용', '하는', '차', '##원', '훈', '##련', '은', '없었다', '##는', '점', '전', '##무', '##하다', '는', '표', '##현', '사', '##용', '된', '이', '##해', '##해', '달', '##라', '고', '윤', '대통령', '옹', '##호', '했다', '윤', '대통령', '확', '##전', '각', '##오', '발', '##언', '국가', '안', '##전', '##보', '##장', '회', '##의', 'NS', '##C', '개', '##최', '하지', '않은', '도', '비', '##판', '올', '##랐다', '설', '##훈', '의', '##원', '은', '확', '##전', '각', '##오', '하고', '무', '##인', '##기', '올', '##려', '보', '##내', '##라고', '대통령', '지', '##시', '했다', '##는데', '저', '##녁', '송', '##년', '##회', '했다', '게', '앞', '##뒤', '맞', '##느', '##냐', '며', '대통령', '전', '##투', '##복', '입', '##고', '[UNK]', '상', '##황', '지', '##켜', '##보', '##고', '있어', '##야', '상', '##식', '맞', '##는', '얘', '##기', '라고', '말', '했다', '송', '##옥', '주', '민', '##주', '##당', '의', '##원', '은', '대통령', '국', '##민', '불', '##안', '증', '##폭', '시', '##킨', '##다고', '하', '##면', '정', '##말', '무', '##책', '##임', '##한', '이라고', '밝혔다', '장', '##관', '은', '에', '##도', '전쟁', '일', '##어', '##나', '##지', '않', '##도록', '하고', '도', '##발', '하지', '못', '하', '##도록', '억', '##제', '하', '##기', '위', '한', '각', '##오', '대', '##비', '해', '##야', '한다', '##는', '라고', '말', '했다', '민', '##주', '##당', '은', '북', '##한', '무', '##인', '##기', '용', '##산', '대통령', '##실', '촬', '##영', '했', '##을', '가', '##능', '##성', '제', '##기', '했다', '김', '##영', '##배', '의', '##원', '은', '국', '##방', '##부', '제', '##공', '한', '북', '##한', '무', '##인', '##기', '항', '##로', '근', '##거', '그', '##림', '만', '보', '##면', '용', '##산', '지', '##나', '##간', '고', '말', '했다', '설', '##훈', '의', '##원', '은', '북', '##한', '용', '##산', '찍', '##은', '용', '##산', '대통령', '##실', '사', '##진', '내', '##놓', '##으면', '할', '이', '##냐', '고', '추', '##궁', '했다', '장', '##관', '은', '용', '##산', '까', '##지는', '오', '##지', '않', '##았', '##던', '은', '확', '##신', '한다', '단', '##계', '별', '##로', '감', '##시', '##자', '산', '##들', '다', '확인', '된다', '고', '답', '했다', '국', '##민', '힘', '은', '북', '##한', '무', '##인', '##기', '격', '##추', '실', '##패', '문', '##재', '##인', '정', '##부', '책', '##임', '있다', '##며', '내', '로', '##남', '##불', '이라고', '주', '##장', '했다', '국', '##방', '위', '여', '##당', '간', '##사', '인', '신', '##원', '##식', '의', '##원', '은', '드', '##디', '##어', '문', '##재', '##인', '정', '##권', '잘', '못', '된', '안', '##보', '정', '##책', '참', '##담', '##한', '성', '##적', '##표', '배', '##달', '됐', '##다', '고', '말', '했다', '신', '의', '##원', '은', '더', '심', '##각', '##한', '게', '9', '19', '합', '##의', '라', '##며', '북', '##한', '은', '대', '##남', '도', '##발', '역', '##량', '자', '##유', '##롭', '##게', '전', '##방', '위', '##로', '강', '##화', '##했', '##는데', '는', '손', '##발', '[UNK]', '묶', '##었다', '고', '말', '했다', '신', '의', '##원', '은', '9', '19', '합', '##의', '설', '##정', '된', '비', '##행', '금', '##지', '구', '##역', '남', '측', '무', '##인', '##기', '운', '##용', '극', '##도', '제', '##한', '하고', '있다', '##며', '무', '##인', '##기', '배', '##치', '들어', '##간', '예', '##산', '410', '##0', '##억', '##원', '그', '##냥', '사', '##장', '시', '##킨', '이다', '김', '##정', '##은', '한', '##테', '잘', '보', '##이', '##기', '위해', '서', '라고', '말', '했다', '같은', '당', '성', '일', '##종', '의', '##원', '은', '9', '19', '합', '##의', '문', '##재', '##인', '정', '##부', 'GP', '일', '##반', '##전', '##초', '다', '헐', '##어', '냈', '##다', '감', '##시', '자', '##산', '설치', '할', '수', '##가', '없는', '라', '##며', '군', '해', '##체', '수', '##준', '가', '##게', '된', '은', '문', '##재', '##인', '정', '##부', '오', '##는', '거', '축', '##적', '된', '이', '##지', '출', '##범', '한', '지', '6', '개', '##월', '밖', '##에', '안', '된', '윤', '##석', '##열', '정', '##부', '책', '##임', '돌', '##린다', '##는', '부', '##끄', '##러', '##운', '줄', '알', '##라', '고', '말', '했다', '민', '##주', '##당', '의', '##원', '반', '##발', '양', '당', '고', '##성', '오', '##갔다', '국', '##민', '힘', '은', '윤', '대통령', 'NS', '##C', '개', '##최', '하지', '않은', '적', '##절', '##한', '판', '##단', '이', '##었다', '##는', '입', '##장', '도', '밝혔다', '한', '##기', '##호', '의', '##원', '은', '작', '##전', '진', '##행', '하고', '있을', '는', '지', '##휘', '관', '모든', '위', '##임', '하는', '라', '##며', 'NS', '##C', '하', '##면', '작', '##전', '지', '##휘', '대통령', '하는', '결과', '된다', '호', '##미', '##로', '막', '걸', '가', '##래', '막', '##는', '이라고', '말', '했다', '신', '의', '##원', '은', '군', '대', '##응', '조', '##치', '북', '##한', '상', '##공', '무', '##인', '##기', '침', '##투', '시', '##킨', '데', '대해', '기', '##념', '##비', '적', '인', '사건', '윤', '##석', '##열', '정', '##부', '휴', '##전', '이후', '북', '##한', '도', '##발', '역', '##사', '종', '##지', '##부', '찍', '수', '있는', '제', '##대로', '된', '창', '썼', '##다', '고', '극', '##찬', '했다', '민', '##주', '##당', '에서', '##는', '대통령', '##실', '용', '##산', '국', '##방', '##부', '청', '##사', '이전', '합', '##참', '흩', '##어져', '전', '##해', '국', '지', '전', '대', '##응', '우', '##려', '커', '##졌다', '##는', '주', '##장', '도', '나', '##왔다', '국', '##민', '힘', '은', '민', '##주', '##당', '이', '##재', '##명', '대', '##표', '방', '##탄', '위해', '사건', '정', '##치', '적', '이', '##용', '한', '다', '고', '맞', '##섰', '##다', '장', '##관', '은', '북', '##한', '무', '##인', '##기', '영', '##공', '침', '##범', '보고', '받은', '언', '인', '##지', '묻', '##는', '민', '##주', '##당', '의', '##원', '처음', '12', '##시', '10', '##분', '이라고', '답', '했다', '##가', '이후', '11', '##시', '50', '##분', '이라고', '말', '했다', '윤', '대통령', '에', '##게', '##는', '12', '##시', '12', '##분', '전', '##화', '보고', '했다', '##고', '장', '##관', '은', '밝혔다', '윤', '대통령', '보고', '시', '##점', '은', '군', '무', '##인', '##기', '처음', '탐', '##지', '한', '오', '##전', '10', '##시', '25', '##분', '으로', '##부터', '1', '##시간', '47', '##분', '지', '##난', '다', '장', '##관', '은', '초기', 'TO', '##D', '열', '상', '##감', '시', '장', '##비', '레', '##이', '##더', '잡', '##히', '##는', '인', '##지', '분', '##석', '하는', '데', '필', '##요', '##하지', '않', '##겠', '##느', '##냐', '고', '말', '했다', '합', '##참', '은', '국', '##방', '위', '보고', '적', '소', '##형', '무', '##인', '##기', '도', '##발', '양', '##상', '고', '##려', '해', '합', '##참', '차', '##원', '통', '##합', '된', '실', '##전', '적', '교', '##육', '훈', '##련', '강', '##화', '##할', '라', '##며', '29일', '합', '##동', '방', '##공', '훈', '##련', '실', '##시', '할', '계', '##획', '이라고', '밝혔다', '훈', '##련', '에서', '##는', '북', '##한', '소', '##형', '무', '##인', '##기', '도', '##발', '상', '##황', '대', '##비', '한', '급', '부', '##대', '별', '탐', '##지', '타', '##격', '자', '##산', '운', '##용', '능', '##력', '중', '##점', '점', '##검', '할', '보인다', '[SEP]']\n"]}]},{"cell_type":"code","source":["# 문장의 최대 시퀀스를 설정하여 정수 인코딩과 제로 패딩을 수행해준다.\n","MAX_LEN = 128 #최대 시퀀스 길이 설정\n","# 정수 인코딩\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","# 제로 패딩\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"],"metadata":{"id":"F7GwcHa8rW8j","executionInfo":{"status":"ok","timestamp":1674551284363,"user_tz":-540,"elapsed":832,"user":{"displayName":"송지빈","userId":"11346694006770834807"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["input_ids[7]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VqHHaTOyrnTv","executionInfo":{"status":"ok","timestamp":1674551284366,"user_tz":-540,"elapsed":28,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"783270db-5a25-43d4-a8dd-e511c4169a10"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   101,  93222,  34951,  30005,  31503,  45670,   9405,  10739,\n","        41605,   8928, 119184,   9866,  30873,   9069, 119187,   9854,\n","        24982,   9668, 118625,   9952,  12310,  19905,   9340,   9670,\n","        14646,   9730,  14279,   9998,  10459,   9569,  17706,  70672,\n","        31503,  59355,   8985,   9603,  21386,  70672,  31503,  66421,\n","         9998,  10459,   9043,   8935,  17138,   9954,  93222,  34951,\n","        30005,  31503,   9657,  10739,   9689,  36210,   9965,  24098,\n","         8928, 119184,   9619,  14279,  14863,   8928, 119184, 105197,\n","        80331,  14279,   8928, 119184,   9356,  34951,   9612,  48556,\n","         8863,  37388,  22333,  93222,  16605,  30005,  14279,   8898,\n","        23321,  12310,  51945,   9670,  30005,  43022,  25387,  14646,\n","         8868,  99118,  40311,   8885,  99118,  40311,   9735,  40958,\n","        23622,   9998,  10459,   9043,   9568,  89523,   9568,   9485,\n","         8928, 119184,   9367,  21711,   9405,  10739,  41605,  28000,\n","        45465,  93222,   9521,  30005,   9619, 119435,   9414,  65649,\n","         9665,  21789,  54780,   9069,  29455,   9960,   9340,   9670])"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## 어텐션 마스크\n","- 0 값을 가지는 패딩 토큰에 대해서 어텐션 연산을 불필요하게 수행하지 않도록 단어와 패딩 토큰을 구분할 수 있게 알려주는 것"],"metadata":{"id":"nHsGDeOqrrnw"}},{"cell_type":"code","source":["attention_masks = []\n","\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)"],"metadata":{"id":"V45JYu8WrtIG","executionInfo":{"status":"ok","timestamp":1674551288208,"user_tz":-540,"elapsed":15,"user":{"displayName":"송지빈","userId":"11346694006770834807"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["print(attention_masks[7])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxaOwXoFs2VS","executionInfo":{"status":"ok","timestamp":1674551291516,"user_tz":-540,"elapsed":6,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"ae6bf973-fbe1-4b07-e7d4-15a397ab79b7"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"]}]},{"cell_type":"markdown","source":["## Train set를 훈련셋과 검증셋으로 분리하기\n","어텐션 마스크도 분리!"],"metadata":{"id":"Lz19fy65s9Ii"}},{"cell_type":"code","source":["labels = train['news'].values\n","# labels = labels.astype(np.int)\n","\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2000, test_size=0.1)\n","\n","train_labels.astype('float64')\n","\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)"],"metadata":{"id":"DNNQb0DYtBhf","executionInfo":{"status":"ok","timestamp":1674551295021,"user_tz":-540,"elapsed":4,"user":{"displayName":"송지빈","userId":"11346694006770834807"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# 배치 사이즈 32로 설정하고, 입력데이터, 어텐션 마스크, 라벨을 하나의 데이터로 묶어 train_dataloader, validation_dataloader라는 입력 데이터를 생성해준다.\n","batch_size = 32\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"metadata":{"id":"qvi-Qz6ottsT","executionInfo":{"status":"ok","timestamp":1674551295804,"user_tz":-540,"elapsed":3,"user":{"displayName":"송지빈","userId":"11346694006770834807"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# Test set 전처리"],"metadata":{"id":"fWh9UrScz4cR"}},{"cell_type":"code","source":["sentences = [\"[CLS] \" + str(s) + \" [SEP]\" for s in test.new_article]"],"metadata":{"id":"4xuMORFQ5k1B","executionInfo":{"status":"ok","timestamp":1674551296317,"user_tz":-540,"elapsed":3,"user":{"displayName":"송지빈","userId":"11346694006770834807"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# [CLS] + 문장 + [SEP]\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","\n","# 라벨 데이터\n","labels = test['news'].values\n","\n","# Word 토크나이저 토큰화\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","# 시퀀스 설정 및 정수 인덱스 변환 & 패딩\n","MAX_LEN = 128\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# 어텐션 마스크\n","attention_masks = []\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","    \n","# 파이토치 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","labels = labels.astype(np.int)\n","test_labels = torch.tensor(labels)\n","test_masks = torch.tensor(attention_masks)\n","\n","# 배치 사이즈 설정 및 데이터 설정\n","batch_size = 32\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BisvVuazz6Iv","executionInfo":{"status":"ok","timestamp":1674551299335,"user_tz":-540,"elapsed":1013,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"7101b9bc-a03e-4643-fb28-b4d4441d7eaf"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-24-91965f42e0e7>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  labels = labels.astype(np.int)\n"]}]},{"cell_type":"code","source":["len(test_inputs), len(test_masks), len(test_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qyruis3V5LWk","executionInfo":{"status":"ok","timestamp":1674551299338,"user_tz":-540,"elapsed":61,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"8ba475f5-3e5b-4049-b448-0b3236e198e2"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100, 100, 100)"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["# BERT 모델 불러오기"],"metadata":{"id":"dTt1UJWrz_fJ"}},{"cell_type":"code","source":["# GPU 설정\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pK6EYkZO4JcU","executionInfo":{"status":"ok","timestamp":1674551299340,"user_tz":-540,"elapsed":37,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"4afa6121-00f4-4b21-962b-39da8a9a927d"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}]},{"cell_type":"code","source":["# pretrain된 BERT 모델을 불러오자\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PP4xUGvM4M3N","executionInfo":{"status":"ok","timestamp":1674551704016,"user_tz":-540,"elapsed":2779,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"077a0961-2ac2-48e3-ba79-31a853399c12"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["model.named_modules"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hn876HQ-CYf1","executionInfo":{"status":"ok","timestamp":1674551748378,"user_tz":-540,"elapsed":328,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"eb074884-a0f7-4a17-8627-43e9916a0dbe"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method Module.named_modules of BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")>"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# 모델 레이어 수정\n","for name, param in model.layer1.named_parameters():\n","    print(name,param.shape,sep=\"   \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"abowmxQUAL_g","executionInfo":{"status":"error","timestamp":1674551322202,"user_tz":-540,"elapsed":15,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"60bd0a0e-f64a-40c4-dd79-bc64fbe2365e"},"execution_count":28,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-3bd86a738680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 모델 레이어 수정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"   \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1270\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'layer1'"]}]},{"cell_type":"code","source":["# 하이퍼 파라미터 설정해주기\n","\n","# 옵티마이저\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률(learning rate)\n","                  eps = 1e-8 \n","                )\n","\n","# 에폭수\n","epochs = 4\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VcDf7xWh4SP1","executionInfo":{"status":"ok","timestamp":1673954164935,"user_tz":-540,"elapsed":51,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"dbf21f65-5f79-44f6-9553-e007b36c52cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# 모델 학습"],"metadata":{"id":"GjxSHjwx4ckf"}},{"cell_type":"code","source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","    \n","    \n","# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"X--eABZ64d8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","#그래디언트 초기화\n","model.zero_grad()\n","\n","# 학습\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ki7MqPup4jyD","executionInfo":{"status":"ok","timestamp":1673945137075,"user_tz":-540,"elapsed":26672,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"1978086d-841c-450b-eda1-ec8c8ec4c876"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.70\n","  Training epcoh took: 0:00:09\n","\n","Running Validation...\n","  Accuracy: 0.47\n","  Validation took: 0:00:00\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.70\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.47\n","  Validation took: 0:00:00\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.70\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.47\n","  Validation took: 0:00:00\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.70\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.37\n","  Validation took: 0:00:00\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["# 테스트셋 평가"],"metadata":{"id":"Rdc3qWVw4xAV"}},{"cell_type":"code","source":["#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTvOlkyZ6Ipg","executionInfo":{"status":"ok","timestamp":1673945199066,"user_tz":-540,"elapsed":1105,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"0acc4f28-107a-4b9a-c4bf-ef534c0629fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.50\n","Test took: 0:00:01\n"]}]},{"cell_type":"markdown","source":["# 새로운 문장 테스트"],"metadata":{"id":"KHT_Eb916Kql"}},{"cell_type":"code","source":["# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 128\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"],"metadata":{"id":"8fIu0TaG6OvG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"],"metadata":{"id":"JOYI5O2H6RO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits = test_sentences(['더 나은 학교생활 하고 싶어'])\n","print(logits)\n","\n","if np.argmax(logits) == 1 :\n","    print(\"연애 관련 대화\")\n","elif np.argmax(logits) == 0 :\n","    print(\"일상 대화\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5J-dEf_h6TCY","executionInfo":{"status":"ok","timestamp":1673945243711,"user_tz":-540,"elapsed":618,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"8acf7ab0-ce3a-4d13-d83b-c0473f139bf5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.17283234 0.16009946]]\n","일상 대화\n"]}]},{"cell_type":"code","source":["logits = test_sentences(['저녁 뭘 먹을지 추천해줘'])\n","\n","print(logits)\n","if np.argmax(logits) == 1 :\n","    print(\"연애 관련 대화\")\n","elif np.argmax(logits) == 0 :\n","    print(\"일상 대화\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUb1pE0D6Vtm","executionInfo":{"status":"ok","timestamp":1673945275276,"user_tz":-540,"elapsed":409,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"9d8cea19-d205-4f60-995f-06ce71e9a6f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.17177275 0.16132557]]\n","일상 대화\n"]}]},{"cell_type":"code","source":["logits = test_sentences(['여자친구한테 선물 뭘로 줄까?'])\n","\n","print(logits)\n","if np.argmax(logits) == 1 :\n","    print(\"연애 관련 대화\")\n","elif np.argmax(logits) == 0 :\n","    print(\"일상 대화\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ViOU4rSQ6dZD","executionInfo":{"status":"ok","timestamp":1673945288263,"user_tz":-540,"elapsed":20,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"68482c54-e087-4ec4-d25a-805e7bdde870"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.17058024 0.15684107]]\n","일상 대화\n"]}]},{"cell_type":"markdown","source":["# KoBert 모델 만들기"],"metadata":{"id":"8pxA74_j6ftf"}},{"cell_type":"markdown","source":["## 1. Colab 환경 설정"],"metadata":{"id":"lX9vKOtF7nVM"}},{"cell_type":"code","source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfN_oc0W6_x_","executionInfo":{"status":"ok","timestamp":1673945647077,"user_tz":-540,"elapsed":148458,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"1f8b9d13-1436-4edb-891d-2d4e1b2d9dec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-uavin68_\n","  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-uavin68_\n","  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting boto3<=1.15.18\n","  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gluonnlp<=0.10.0,>=0.6.0\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting mxnet<=1.7.0.post2,>=1.4.0\n","  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n","  Downloading onnxruntime-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n","  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n","  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m205.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1102397440 bytes == 0x3a908000 @  0x7fe15a58e615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n","  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n","Collecting s3transfer<0.4.0,>=0.3.0\n","  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting botocore<1.19.0,>=1.18.18\n","  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.32)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.25.1)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.9.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n","Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n","Building wheels for collected packages: kobert, gluonnlp, sacremoses\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=117719b0c65e96a9163f52c463a268198c81c618344f165e6d742d22e8777b38\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-uswvmlss/wheels/bf/5f/74/81bf3a1332130eb6629ecf58876a8746b77021e7d7b0638e91\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp38-cp38-linux_x86_64.whl size=619643 sha256=75fda1e7d2f05fb9880e0b9761f4d9b2ae29762da49f856e2ce9297a7ede00a3\n","  Stored in directory: /root/.cache/pip/wheels/b6/93/9d/2237550c409eb3ed725d6302b7897ddd9a037b40cef66dcd9c\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=a650d743189c609db885f72e2bca2f43f6a14f519156fc4067a5d625a4ad8815\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built kobert gluonnlp sacremoses\n","Installing collected packages: tokenizers, sentencepiece, torch, sacremoses, onnxruntime, jmespath, graphviz, mxnet, huggingface-hub, gluonnlp, botocore, transformers, s3transfer, boto3, kobert\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.2\n","    Uninstalling tokenizers-0.13.2:\n","      Successfully uninstalled tokenizers-0.13.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0+cu116\n","    Uninstalling torch-1.13.0+cu116:\n","      Successfully uninstalled torch-1.13.0+cu116\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.11.1\n","    Uninstalling huggingface-hub-0.11.1:\n","      Successfully uninstalled huggingface-hub-0.11.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.25.1\n","    Uninstalling transformers-4.25.1:\n","      Successfully uninstalled transformers-4.25.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.10.1 which is incompatible.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.10.1 which is incompatible.\n","torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed boto3-1.15.18 botocore-1.18.18 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","\n","#kobert\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","#transformers\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"metadata":{"id":"VN7ILC3D68tQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#BERT 모델, Vocabulary 불러오기\n","bertmodel, vocab = get_pytorch_kobert_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lftR-vnT6999","executionInfo":{"status":"ok","timestamp":1673945735139,"user_tz":-540,"elapsed":4503,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"ea3347c4-bb5c-4bb9-ff27-27e11f01988f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["using cached model. /content/.cache/kobert_v1.zip\n","using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"]}]},{"cell_type":"markdown","source":["## 2. 데이터셋 전처리"],"metadata":{"id":"W1dV06hE7lfP"}},{"cell_type":"markdown","source":["## 3.Train data & Test data"],"metadata":{"id":"vaGsP1sE8mo8"}},{"cell_type":"code","source":["print(len(train))\n","print(len(test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LilPwJDn8INX","executionInfo":{"status":"ok","timestamp":1673945811607,"user_tz":-540,"elapsed":377,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"85c730b4-4c44-4e0f-a73f-61acbcbb83d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["300\n","100\n"]}]},{"cell_type":"markdown","source":["## 4.KoBERT 입력 데이터로 만들기"],"metadata":{"id":"6MFdPh308gP5"}},{"cell_type":"code","source":["train[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"8yBxvbydDWs8","executionInfo":{"status":"ok","timestamp":1673947611446,"user_tz":-540,"elapsed":12,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"a67932d6-91f9-4cb7-dc87-592e11b75ef3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                      title  \\\n","247      허은아 “친윤이고 검사 출신이면 당협 쇼핑하는 현실 부끄럽다”   \n","110                 與 당권주자들, 새해벽두부터 줄줄이 출정식   \n","16     與조강특위, 조직위원장 42명 선임…‘이재명 저격수’ 장영하 포함   \n","66      尹 “전쟁 준비” 발언에…이재명 “안보무능 정권의 철부지 행동”   \n","153  野 “1월 임시국회 소집 불가피”…與 “이재명 방탄국회 열겠다는 것”   \n","\n","                                           new_article news  \n","247  은 국민 힘 의원 29일 이준석 대표 시절 내정 됐던 서울 동대문 당원 협의 회 당...    0  \n","110  권성동 1월 6일 출마 선언 계획 안철수 늦어도 설연휴 전 밝힐듯 2023년 새해 ...    1  \n","16   국민 힘 조직 강화 특별 위원회 조강 특위 는 29일 총 42 명의 국회의원 선거구...    1  \n","66   민주당 윤석열 대통령 전쟁 준비 발언 두고 안보 불안 부추기고 며 공세 나섰다 이재...    1  \n","153  이재명 민주당 대표 박찬 대 최고 위원 지난 23일 오후 서울 여의도 국회 열린 본...    1  "],"text/html":["\n","  <div id=\"df-b28bb9ed-155c-4843-bcdb-772e35e56443\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>new_article</th>\n","      <th>news</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>247</th>\n","      <td>허은아 “친윤이고 검사 출신이면 당협 쇼핑하는 현실 부끄럽다”</td>\n","      <td>은 국민 힘 의원 29일 이준석 대표 시절 내정 됐던 서울 동대문 당원 협의 회 당...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>110</th>\n","      <td>與 당권주자들, 새해벽두부터 줄줄이 출정식</td>\n","      <td>권성동 1월 6일 출마 선언 계획 안철수 늦어도 설연휴 전 밝힐듯 2023년 새해 ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>與조강특위, 조직위원장 42명 선임…‘이재명 저격수’ 장영하 포함</td>\n","      <td>국민 힘 조직 강화 특별 위원회 조강 특위 는 29일 총 42 명의 국회의원 선거구...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>尹 “전쟁 준비” 발언에…이재명 “안보무능 정권의 철부지 행동”</td>\n","      <td>민주당 윤석열 대통령 전쟁 준비 발언 두고 안보 불안 부추기고 며 공세 나섰다 이재...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>153</th>\n","      <td>野 “1월 임시국회 소집 불가피”…與 “이재명 방탄국회 열겠다는 것”</td>\n","      <td>이재명 민주당 대표 박찬 대 최고 위원 지난 23일 오후 서울 여의도 국회 열린 본...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b28bb9ed-155c-4843-bcdb-772e35e56443')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b28bb9ed-155c-4843-bcdb-772e35e56443 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b28bb9ed-155c-4843-bcdb-772e35e56443');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":190}]},{"cell_type":"code","source":["# 토큰화 & 패딩\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","\n","        labels = test['news'].values\n","        self.labels = labels.astype(np.int)\n","\n","        # self.labels = [np.int32(i[label_idx]) for i in dataset]\n","        \n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"],"metadata":{"id":"27Pk76En8pM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 하이퍼파라미터 조정 Batch size는 64, epochs는 5, learning rate는 5e-5\n","# Setting parameters\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"],"metadata":{"id":"wf0eSrKs8zAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#토큰화\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","data_train = BERTDataset(train, 1, 2, tok, max_len, True, False)\n","data_test = BERTDataset(test, 1, 2, tok, max_len, True, False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRm23V2u86OF","executionInfo":{"status":"ok","timestamp":1673948057932,"user_tz":-540,"elapsed":24,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"250e45cd-8e2c-4f28-a7fa-314a6d8e9b39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-192-e65f04dac005>:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  self.labels = labels.astype(np.int)\n"]}]},{"cell_type":"code","source":["# 첫 번째는 패딩된 시퀀스, 두 번째는 길이와 타입에 대한 내용, 세 번째는 어텐션 마스크 시퀀스\n","data_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbxLg6Zw8-TL","executionInfo":{"status":"ok","timestamp":1673948125170,"user_tz":-540,"elapsed":25,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"07f4abed-60b9-4a0d-c2d4-b70e971ce6f1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([  2, 517, 405,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n","       dtype=int32),\n"," array(4, dtype=int32),\n"," array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","       dtype=int32),\n"," 0)"]},"metadata":{},"execution_count":196}]},{"cell_type":"code","source":["# torch 형식의 dataset을 만들어준다.\n","train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztWRDpoPFVII","executionInfo":{"status":"ok","timestamp":1673948152960,"user_tz":-540,"elapsed":25,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"bfd4994f-09cc-4aca-88a3-03f6dca185c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  assert self._num_workers == 0\n"]}]},{"cell_type":"markdown","source":["## 5. KoBERT 학습 모델 만들기"],"metadata":{"id":"GfB1VFjcFb7L"}},{"cell_type":"code","source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=2,   ##클래스 수 조정##\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"metadata":{"id":"06MusJpfFfrO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#BERT 모델 불러오기\n","model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n","\n","#optimizer와 schedule 설정\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","\n","#정확도 측정을 위한 함수 정의\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","    \n","train_dataloader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xp4z_OwKFsSH","executionInfo":{"status":"ok","timestamp":1673948295196,"user_tz":-540,"elapsed":22,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"e15737a9-6640-4fc1-d6cc-d289ed1ca94f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7f84ebe49340>"]},"metadata":{},"execution_count":203}]},{"cell_type":"markdown","source":["## 6. KoBERT 모델 학습시키기"],"metadata":{"id":"OC_2DH3kFus4"}},{"cell_type":"code","source":["for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    \n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542,"referenced_widgets":["59d548d312cc434a91b4edc804f92818","8144706f38d543aca4caa246ea2cb3dd","649a5c1b98764380a75af0fd8c465d5f","bfd35328b164419f9a7e106e11ee0923","5be6aa90465240dfb6707e3d28c514db","a61914cc54124ebaa4c532271f421944","f7a842211be9411ebfc8ab6681362b1f","4eb6ca548bdb4dcb89540c8c1b8342aa","983816f06fe7419ca083822805baad69","b32dbdf975104f7aa60518a286a3dfbc","fd3149320c39457ea43c049373b561f0"]},"id":"MFhQR4NPGCsk","executionInfo":{"status":"error","timestamp":1673948319822,"user_tz":-540,"elapsed":623,"user":{"displayName":"송지빈","userId":"11346694006770834807"}},"outputId":"3ecb01be-8210-43ef-dd32-fc1c32650cc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-204-480b6a139979>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d548d312cc434a91b4edc804f92818"}},"metadata":{}},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-204-480b6a139979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;31m#      `_utils.python_exit_status`. Since `atexit` hooks are executed in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;31m#      reverse order of registration, we are guaranteed that this flag is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m     \u001b[0;31m#      set before library resources we use are freed (which, at least in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m     \u001b[0;31m#      CPython, is done via an `atexit` handler defined in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;31m#      `multiprocessing/util.py`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    del data, idx, index, r  # save memory\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n  File \"<ipython-input-192-e65f04dac005>\", line 16, in __getitem__\n    return (self.sentences[i] + (self.labels[i], ))\nIndexError: list index out of range\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8eq9k6Q4GEoI"},"execution_count":null,"outputs":[]}]}